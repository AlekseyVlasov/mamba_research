{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c22d4720-c7a0-479a-8237-3306fd1eb8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c530615-c0a2-4800-a2c9-34f49a9a200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import pytorch_warmup as warmup\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from mamba_ssm.models.config_mamba import MambaConfig\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import print_model_size, fix_seed\n",
    "from models.MambaWithEmbeddings import MambaLMHeadModelWithEmbeddings\n",
    "from train_yelp_reviews import add_special_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54545f0f-251f-452b-8d4b-1bd5b6f1f574",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"yelp_polarity\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "160b4663-5ea5-4309-a4e2-cf5c7984eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "model_name = \"state-spaces/mamba-130m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80a24ae4-ce45-407b-a76c-20e2a718ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(seed)\n",
    "model = MambaLMHeadModelWithEmbeddings.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84b6c962-1688-4f48-8d09-ae9428600d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1c37ad25-d079-4dc6-a735-b9da7369d585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1919, -0.0347,  0.0941,  ..., -0.1357,  0.1383, -0.0218],\n",
       "        [ 0.0317,  0.0470, -0.2262,  ..., -0.1767,  0.0739,  0.0479],\n",
       "        [-0.0528,  0.1756,  0.1555,  ...,  0.2199, -0.1916, -0.0371],\n",
       "        ...,\n",
       "        [-0.1702, -0.0906, -0.0830,  ..., -0.0135, -0.0842,  0.0020],\n",
       "        [ 0.0432,  0.0286, -0.1470,  ...,  0.0472,  0.0670, -0.1609],\n",
       "        [ 0.1678,  0.0724,  0.0571,  ..., -0.1032, -0.0364,  0.0401]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.backbone.layers[-1].mixer.out_proj.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e3eb43c5-9afa-400f-900f-6da7fc1b085d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0009,  0.0087,  0.0269,  ...,  0.0269, -0.0181,  0.0178],\n",
       "        [-0.0118,  0.0232,  0.0009,  ...,  0.0321, -0.0164, -0.0025]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classification_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c4877a18-b762-4d2b-b298-26be9a01ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_number = 0\n",
    "device = torch.device(f'cuda:{gpu_number}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ca99528-aef2-40ca-b239-d054f32d66b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "learning_rate = 1\n",
    "tokens_num = 100\n",
    "period = 50\n",
    "warmup_percent = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "96a3cf84-12b7-4ff7-9c23-dde94f111198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:   0%|                                                                                            | 0/35000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Transfer: 0.0002s\n",
      "Embedding Conversion: 0.0004s\n",
      "Add Special Token: 0.0007s\n",
      "Forward Pass: 0.4747s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:   0%|                                                        | 1/35000 [00:01<10:50:33,  1.12s/it, Train Loss (batch)=1.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Pass & Optimization: 0.5788s\n",
      "Scheduler Update: 0.0001s\n",
      "Total Batch Time: 1.0602s\n",
      "Data Transfer: 0.0001s\n",
      "Embedding Conversion: 0.0002s\n",
      "Add Special Token: 0.0005s\n",
      "Forward Pass: 0.3824s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:   0%|                                                        | 2/35000 [00:02<10:01:09,  1.03s/it, Train Loss (batch)=1.48]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Pass & Optimization: 0.5732s\n",
      "Scheduler Update: 0.0001s\n",
      "Total Batch Time: 0.9592s\n",
      "Data Transfer: 0.0001s\n",
      "Embedding Conversion: 0.0002s\n",
      "Add Special Token: 0.0004s\n",
      "Forward Pass: 0.3825s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:   0%|                                                         | 3/35000 [00:03<9:45:24,  1.00s/it, Train Loss (batch)=1.32]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Pass & Optimization: 0.5745s\n",
      "Scheduler Update: 0.0001s\n",
      "Total Batch Time: 0.9602s\n",
      "Data Transfer: 0.0001s\n",
      "Embedding Conversion: 0.0001s\n",
      "Add Special Token: 0.0004s\n",
      "Forward Pass: 0.3825s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:   0%|                                                         | 4/35000 [00:04<9:38:16,  1.01it/s, Train Loss (batch)=1.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Pass & Optimization: 0.5757s\n",
      "Scheduler Update: 0.0001s\n",
      "Total Batch Time: 0.9612s\n",
      "Data Transfer: 0.0001s\n",
      "Embedding Conversion: 0.0001s\n",
      "Add Special Token: 0.0004s\n",
      "Forward Pass: 0.3811s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:   0%|                                                         | 5/35000 [00:05<9:34:43,  1.01it/s, Train Loss (batch)=1.04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Pass & Optimization: 0.5763s\n",
      "Scheduler Update: 0.0001s\n",
      "Total Batch Time: 0.9605s\n",
      "Data Transfer: 0.0001s\n",
      "Embedding Conversion: 0.0001s\n",
      "Add Special Token: 0.0004s\n",
      "Forward Pass: 0.3849s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:   0%|                                                         | 6/35000 [00:05<9:32:28,  1.02it/s, Train Loss (batch)=0.95]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Pass & Optimization: 0.5748s\n",
      "Scheduler Update: 0.0001s\n",
      "Total Batch Time: 0.9628s\n",
      "Data Transfer: 0.0001s\n",
      "Embedding Conversion: 0.0001s\n",
      "Add Special Token: 0.0004s\n",
      "Forward Pass: 0.3831s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:   0%|                                                        | 7/35000 [00:06<9:30:58,  1.02it/s, Train Loss (batch)=0.892]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Pass & Optimization: 0.5759s\n",
      "Scheduler Update: 0.0001s\n",
      "Total Batch Time: 0.9625s\n",
      "Data Transfer: 0.0001s\n",
      "Embedding Conversion: 0.0002s\n",
      "Add Special Token: 0.0004s\n",
      "Forward Pass: 0.3843s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:   0%|                                                        | 8/35000 [00:07<9:30:15,  1.02it/s, Train Loss (batch)=0.776]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Pass & Optimization: 0.5760s\n",
      "Scheduler Update: 0.0001s\n",
      "Total Batch Time: 0.9635s\n",
      "Data Transfer: 0.0001s\n",
      "Embedding Conversion: 0.0001s\n",
      "Add Special Token: 0.0004s\n",
      "Forward Pass: 0.3839s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:   0%|                                                         | 9/35000 [00:08<9:38:08,  1.01it/s, Train Loss (batch)=1.27]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Pass & Optimization: 0.5764s\n",
      "Scheduler Update: 0.0001s\n",
      "Total Batch Time: 0.9633s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "special_token = torch.randn(1, tokens_num, model.config.d_model, requires_grad=True, device=device)\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    [{'params': [special_token], 'lr': learning_rate},\n",
    "     {'params': model.parameters(), 'lr': learning_rate}],\n",
    ")\n",
    "\n",
    "total_steps = num_epochs * len(train_dataloader)\n",
    "warmup_steps = int(warmup_percent * total_steps)  # Calculate warmup steps as a percentage of total steps\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=5e-6)\n",
    "warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_steps)\n",
    "\n",
    "fix_seed(seed)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to evaluation mode\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "    for batch in pbar:\n",
    "        i += 1\n",
    "\n",
    "        if i == 10:\n",
    "            break\n",
    "        batch_start_time = time.time()\n",
    "        torch.cuda.synchronize()\n",
    "        inputs, labels = batch['input_ids'], batch['labels']\n",
    "        \n",
    "        # Move data to the specified device\n",
    "        start = time.time()\n",
    "        torch.cuda.synchronize()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"Data Transfer: {time.time() - start:.4f}s\")\n",
    "\n",
    "        # Convert inputs to embeddings without tracking gradients\n",
    "        start = time.time()\n",
    "        torch.cuda.synchronize()\n",
    "        with torch.no_grad():\n",
    "            embedded_inputs = model.backbone.embedding(inputs)\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"Embedding Conversion: {time.time() - start:.4f}s\")\n",
    "\n",
    "        # Add special token\n",
    "        start = time.time()\n",
    "        torch.cuda.synchronize()\n",
    "        embedded_with_special = add_special_token(embedded_inputs, special_token, period, tokens_num)\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"Add Special Token: {time.time() - start:.4f}s\")\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        start = time.time()\n",
    "        torch.cuda.synchronize()\n",
    "        outputs = model(embedded_with_special, is_embeds=True, num_last_tokens=1)\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"Forward Pass: {time.time() - start:.4f}s\")\n",
    "        \n",
    "        logits = outputs.logits[:, 0, :]\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        start = time.time()\n",
    "        torch.cuda.synchronize()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"Backward Pass & Optimization: {time.time() - start:.4f}s\")\n",
    "        \n",
    "        # Apply warmup and scheduler updates\n",
    "        start = time.time()\n",
    "        torch.cuda.synchronize()\n",
    "        with warmup_scheduler.dampening():\n",
    "            scheduler.step()\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"Scheduler Update: {time.time() - start:.4f}s\")\n",
    "\n",
    "        # Accumulate training loss and accuracy\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = logits.max(1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Calculate local train accuracy\n",
    "        train_accuracy_local = 100 * correct_train / total_train\n",
    "        \n",
    "        # Display the current loss for each training batch\n",
    "        pbar.set_postfix({\"Train Loss (batch)\": loss.item()})\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"Total Batch Time: {time.time() - batch_start_time:.4f}s\")\n",
    "    break\n",
    "\n",
    "    # ---- Epoch-Based Logging ----\n",
    "    train_accuracy_epoch = 100 * correct_train / total_train\n",
    "\n",
    "    # ---- Validation phase after each epoch ----\n",
    "    val_batch_losses_local, val_accuracy = inference(\n",
    "        model, val_loader, device, criterion=criterion, num_last_tokens=1, special_token=special_token, period=period\n",
    "    )\n",
    "    val_loss = sum(val_batch_losses_local) / len(val_batch_losses_local)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9cf3f924-36df-465b-9f79-6b461d1e38a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 768])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_token.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3801feaa-cb20-423d-ab1d-f9c777a3d296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters number: 129136898\n",
      "Model size: 492.62 MB\n",
      "Model size: 0.48 GB\n"
     ]
    }
   ],
   "source": [
    "print_model_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e4f5996-4973-4b59-a628-d5c29c2faabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters number: 1538\n",
      "Model size: 0.01 MB\n",
      "Model size: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "print_model_size(model.classification_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b11fc3ed-478f-4fcb-88c0-c3ff8bdf0e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e5818fb0-0241-4138-87ab-825adf86d445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34eb247-cbf1-4027-9dbd-6cf698a54d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mamba_env)",
   "language": "python",
   "name": "mamba_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
