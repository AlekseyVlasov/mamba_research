gpu_number: 1
seed: 1234
save_model: False

wandb:
  project: "mamba-research"
  group: "yelp_reviews"
  name: "lora"

model:
  name: "state-spaces/mamba-130m"
  lora:
    r: 1
    lora_alpha: 2
    lora_dropout: 0.1
    bias: "none"
    # target_modules: ["in_proj", "x_proj", "dt_proj", "out_proj"]
    # target_modules: ["in_proj", "out_proj"]
    target_modules: ["out_proj"]

training_type: "lora"

training:
  test_batch_size: 16
  batch_size: 16
  accumulation_steps: 1
  epochs: 3
  learning_rate: 0.0001
  warmup_percent: 0.05
